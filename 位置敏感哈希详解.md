# 位置敏感哈希详解

---

>  Locality Sensitive Hashing

---

## 1.Nearest Neighbor Search

NNS: 给定一个查询q，在指定的数据集中返回与q最‘近邻’或‘相似’的记录。

**应用场景：**
- 1.机器学习、数据挖掘及信息检索。
- 2.图像、音频检索：通常图像、音频文件都比较大，并且比较起来相对麻烦，我们可以事先对其计算LSH，用作信息指纹，这样可以给定一个文件的LSH值，快速找到与其相等或相近的图像和文件。
- 3. 聚类：将LSH值作为样本特征，将相同或相近的LSH值的样本合并在一起作为一个类别

**传统方法：** R-tree、KD-Tree、SR-Tree、SIFT、BBF等

``传统方法返回的结果是精确的，但是在高维数据集上的时间效率并不高。一般在维度高于10之后，基于空间划分的算法时间复杂度反而不如线性查找``

NNS面临的三大挑战：
- Curse of Dimensionality
- Storage cost
- Query speed

``由于传统方法是通过brute-force algorithm对所有样本点进行逐一匹配，因此人们开始探索一种approximate nearest neighbors problem的解决方法``

---

## 2.位置敏感哈希

**LSH的作用:**
LSH方法能够在保证一定程度上的准确性的前提下，时间和空间复杂度得到降低，并且能够很好地支持高维数据的检索。

如下图，空间上的点经位置敏感哈希函数散列之后，对于q，其rNN有可能散列到同一个桶（如第一个桶）,即散列到第一个桶的概率较大，会大于某一个概率阈值p1；而其(1 + emxilong) rNN之外的对象则不太可能散列到第一个桶，即散列到第一个桶的概率很小，会小于某个阈值p2。

[LSH_explain.png](https://yunpan.oa.tencent.com/note/api/file/getAttach?fileId=5955cb9d6f0b933205037e54)

### 2.1 LSH定义

对于任意q, p属于X，若从集合X到U的函数族：$ H = \{ h : X -> U \} $ 对距离函数 $ D(q, p) $，如欧式距离、曼哈顿距离、汉明距离等等，满足条件：

若 $ D(p, q) \leq r_1 $, 且 $ Pro[h(p) = h(q)] \geq p_1 $
若 $ D(p, q) > r_2 $, 且 $ Pro[h(p) = h(q)] \leq p_2 $
$ p_1 > p_2; p_1, p_2 \in (0, 1) $
$ r_2 = c \cdot r_1; c > 1 $

则称： $ (r_1, r_2, p_1, p_2) $ 对 $ D(p, q) $ 是位置敏感的。

$ \rho = \frac{\log_2(\frac{1}{p_1})}{\log_2(\frac{1}{p_2})} $

时间复杂度： $ O(\frac{n^\rho}{p_1} ) $

空间复杂度： $ O(\frac{n^{1+\rho}}{p_1}) $

查询失败概率最大： $ \frac{1}{3} + \frac{1}{e} < 1 $

### 2.2 LSH算法：

定义一个哈希映射关系：
$ G = {g : X -> U^k} $
从$ G $中选择$ L $个哈希族： $ g_1, g_2, \cdots, g_L $
每个哈希族中： $ g(p) = ( h_{i1}(p), h_{i2}(p), \cdots, h_{ik}(p) );  h_{it} \in H $
从计算的bucket中最大选取 $ 3 \cdot L $个‘近邻’点，进行计算，最终选取 $ top^K $

---

**Algorithm** Preprocessing:
**Input** A set of points P,
　　$ l $ ( number of hash tables )
**Output** Hash tables $ T_i, i = 1, \dots, l $
**Foreach** $ i = 1, \dots, l $
　　Initialize hash table $ T_i $ by generating a random hash function $ g_i( . ) $
**Foreach** $ i = 1, \dots, l $
　　**Foreach** $ j = 1, \dots, n $
　　　　Store point $ p_j $ on bucket $ g_i(p_j) $ of hash table $ T_i $

---

**Algorithm** Approximate Nearest Neighbor Query
**Input** A query point $ q_i $,
　　$ K $ (number of appr. nearest neighbors)
**Access** To hash tables $ T_i, i = 1, \dots, l $
　　generated by the prepocessing algorithm
**Output** $ K $ (or less) appr. nearest neighbors
$ S <= NULL $
**Foreach** $ i = 1, \dots, l $
　　$ S <= S \cup \{ $ points found in $ g_i(q) $ bucket of table $ T_i \} $
Return the $ K $ nearest neighbors of $q $ found in set S

---

### 2.3 LSH参数：

#### 2.2.1 $ L $ 参数：

定义 $ \delta $，其中一个近邻点被报告的概率至少为： $ 1 - \delta $
$ R $ 为半径，近邻点 $ v \in B(q, R) $，两点在一个哈希族中($ g $函数，$ L $ 个)某个桶中发生碰撞，满足：
　　$ Pro[g(q) = g(v)] \geq p_1 $
则，$ q $ 和 $ v $ 在$ L $ 个 $ g $ 函数中的值都不想等的概率最多为：
　　$ (1 - p_1^k)^L $
则，$ q $ 在至少一个 $ g $ 函数中与 $ v $的值相等的概率满足：
　　$ 1 - (1 - p_1^k)^L \geq 1 - \delta $

$ L $ 满足：
　　$ L \geq \frac{log(\delta)}{log(1 - p_1^k)}  $

``因此``，选择参数 $ L $：
　　$ L = \lceil \frac{log(\delta)}{log(1 - p_1^k)} \rceil  $

#### 2.2.2 $ k $ 参数：

LSH查询时间分两部分：
- 计算 $ q $ 的$ L $ 个 $ g_i $ $ (1 \leq i \leq L) $ 的时间: 
　　$ T_g = O(d \cdot k \cdot L) $
- 计算各哈希表 $ g_i(p) $ 中发生碰撞桶中的点与查询点 $ q $ 之间距离的时间： 
　　$ T_c = O(d \cdot \#collisions) $
　　其中 $ \#collisions $ 是碰撞桶的数目，均值为： $ E(\#collisions) = L \cdot \sum_{v \in P}(p^k \cdot (||q -v||_2)) $

可知，$ T_g $ 随 $ k $ 的增大而增大，而 $ T_c $ 随 $ k $ 的增大而减小，因此存在一个 $ k $ 的最优值使 $ T_g + T_c $ 最小

``实现机制:``
从查询集中随机选取一组样本点组成集合 $ S $，构造一个样本数据结构，并在这个数据结构上执行多次查询，测得一组真实的 $ T_g $ 与 $ T_c $ 的值，计算出 $ T_c $ 的平均值 $ T_c^` $；最终得出：
　　$ T_c = \frac{\sum_{q \in S}(T_c^`(q))}{|S|} $
最终，选取对应 $ T_g + T_c^` $ 最小的 $ k $ 值。

### 2.4 LSH加速

当前，基本LSH存在两方面加速优化：
- hash函数快速计算优化：以减少hash函数数量以达到快速计算
- 分桶hash策略优化：以减少内存使用并达到一定加速效果

#### 2.4.1 Hash快速计算

在基本LSH算法中， $ L $ 个哈希函数 $ g_i = ( h_1^i, h_2^i, \dots, h_k^i ) $ 是随机独立生成的；在对单点 $ q $ 的计算中，需要哈希处理 $ k * L $ 次。对此，可以对哈希函数的使用做优化。

- 设 $ k $ 为偶数， $ m $ 为一个常量，定义函数： $ u_i = ( h_1^i, h_2^i, \dots, h_\frac{k}{2}^i ) $
- 定义哈希族： $ g_i = ( u_a, u_b ) $； $ 1 \leq a < b \leq m $

总共产生哈希族的个数： $ L = \frac{m \cdot (m - 1)}{2}$

$ m $ 个 $ u $ 函数中， 查询点和计算点哈希值都不相等的概率： $ (1 - p_1^\frac{k}{2})^m $

只有一个 $ u $ 函数中，查询点和计算点的哈希值相等的概率： $ m \cdot p_1^\frac{k}{2} \cdot (1 - p_1^\frac{k}{2})^{m-1} $

因此，在LSH加速方案下，查询点与计算点在至少一个哈希族 $ g $ 中相等的概率满足：
$ 1 - (1 - p_1^\frac{k}{2})^m - m \cdot p_1^\frac{k}{2} \cdot (1 - p_1^\frac{k}{2})^{m-1} \geq 1 - \delta $

计算得出的 $ L $ 会略大于独立哈希族时的 $ L $， 但依然满足 $ O(\frac{log{\frac{1}{\delta}}}{p_1^k}) $ ； 而哈希计算的时间复杂度降低到： $ T_g = O(d \cdot k \cdot L) = O(d \cdot k \cdot \sqrt{L}) $ 

其中，$ T_c $ 不变。

#### 2.4.2 哈希分桶加速

在基本LSH中，单个哈希族 $ g_i(v) $ 对应哈希表的key是一个 $ k $ 维的"数组"，每个值是族内对应一个哈希函数对查询点 $ q $ 的计算结果；因此，哈希表即占用内存，又不利于高效查询。

对哈希分桶加速采取额外构造两个传统hash函数：
-  $ h_1 $ 计算哈希表的索引： $ h_1(a_1, a_2, \dots, a_k) = ((\sum_{i=1}^k r_i^1 \cdot a_i) \mod C_{prime}) \mod tableSize $
- $ h_2 $ 计算哈希表中桶的索引： $ h_2(a_1, a_2, \dots, a_k) = (\sum_{i=1}^k r_i^2 \cdot a_i) \mod C_{prime} $

$ C_{prime} $： 素数 $ 2^{32} - 5$
$ tableSize $: 哈希表大小，32bit整形
$ r_i^1 $ 和 $ r_i^2 $： 随机整数向量
$ r_i^1 \cdot a_i $：64bit整形

根据新加入的两个哈希函数来看，执行流程：
- 0.构造哈希数组： $ tableSize \cdot C_{prime} $
- 1.在基本LSH中，在计算每个LSH哈希族后得到原始参数： $ (a_1, a_2, \dots, a_k) = g_i(x_1, x_2, \dots, x_k) $
- 2.根据 $ h_1 $ 计算出中间哈希表的索引，并取出哈希表对象
- 3.根据 $ h_2 $ 计算出哈希表中桶的索引，取出桶中对应的候选向量信息

---

## 3.LSH发展历程

- 1998年, P.Indy和R.Motwani提出了LSH算法的理论基础。
- 1999 年Gionis A,P.Indy和R.Motwani使用哈希的办法解决高维数据的快速检索问题, 这也是Basic LSH算法的雏形。
- 2004 年, P.Indy 提出了LSH 算法在欧几里德空间(2-范数)下的具体解决办法。
- 同年, 在自然语言处理领域中, Deepak Ravichandran使用二进制向量和快速检索算法改进了Basic LSH 算法 [8], 并将其应用到大规模的名词聚类中, 但改进后的算法时间效率并不理想。
- 2005 年, Mayank Bawa, Tyson Condie 和Prasanna Ganesan 提出了LSH Forest算法[9], 该算法使用树形结构代替哈希表, 具有自我校正参数的能力。
- 2006 年, R. Panigrahy[10]用产生近邻查询点的方法提高LSH 空间效率, 但却降低了算法的空间效率。
- -2007年,William Josephson 和Zhe Wang使用多重探测的方法改进了欧几里德空间(2-范数)下的LSH 算法, 同时提高了算法的时间效率和空间效率。

**LSH分类:**
- 1.基于随机超平面投影的方法
- 2.Cross-polytype
- 3.SimHash
- 4.Kernel LSH

当前，FALCONN实现了基于余弦距离的Hyperplane和Cross Polytope，同时也可以使用L2距离运行。

---

## 4.随机超平面投影法（Random Hyperplane Projection）

大神Charikar改进了上种方法的缺点，提出了一种随机超平面投影LSH.
这种方法的最大优点在于：
1),不需要参数设定
2),是两个向量间的cosine距离，非常适合于文本度量
3),计算后的value值是比特形式的1和0，免去了前面算法的再次变化

在n维空间中随机选取一个非零向量：$ x = \{ x_1, x_2, \dots, x_n \} $ 
考虑以该向量为法向量且经过坐标系原点的超平面，该超平面把整个n维空间分成了两部分，将法向量所在的空间称为正空间，另一空间为负空间。那么集合S中位于正空间的向量元素hash值为1，位于负空间的向量元素hash值为0。判断向量属于哪部分空间的一种简单办法是判断向量与法向量之间的夹角为锐角还是钝角，因此具体的定义公式可以写为

---

## 5.Cross-polytope

### 5.1 简单的交叉Ploytope

暂无。

### 5.2 Multi-Probe LSH

多探寻的局部敏感哈希，由Qin Lv、William Josephson、Zhe Wang、Moses Charikar、Kai Li在发表于VLDB 2007中的论文《MultiProbe LSH: Efficient Indexing for HighDimensional Similarity Search》中提出。该方法针对基本LSH方法需要查询大量哈希表来保证搜索质量的缺点进行优化，其核心思想是**使用一个严格挑选的探测序列来检测多个可能包含最近邻点的桶。根据lsh的性质，如果查询点q的近邻p没有被映射到q所在的桶，那么p很可能在p所在桶附近的桶中。

通过实验证明，在保证相同时间效率的情况下，Multi-Probe LSH比基本的LSH方法减少了一个数量级的哈希表数量；在保证相同的搜索质量的情况下，Multi-Probe LSH比基于熵的LSH方法（Entropy-based LSH）减少了查询时间，同时少用了5-8倍的哈希表。

对于查询点 $ q $，定义一个扰动序列向量：
　　$ \Delta = \{ \delta_1, \delta_2, \dots, \delta_m \} $
　　扰动范围： $ \Delta \in \{ -1, 0, 1 \} $
对于每个哈希桶应用：
　　$ g(q) + \Delta $

---

## 6.SimHash

暂无。

---

## 7.Kernel LSH

对于某些特定情况还是不行：比如输入的key值不是均匀分布在整个空间中，可能只是集中在某个小区域内，需要在这个区域内放大距离尺度。又比如我们采用直方图作为特征，往往会dense一些，向量只分布在大于0的区域中，不适合采用cosine距离，而stable Distribution投影方法参数太过敏感，实际设计起来较为困难和易错，不免让我们联想，是否有RBF kernel这样的东西能够方便的缩放距离尺度呢？或是我们想得到别的相似度表示方式。这里就需要更加fancy的kernel LSH了。

---

## 8.Hamming Metric

使用汉明距离LSH

$ \rho = \frac{\log_2(\frac{1}{p_1})}{\log_2(\frac{1}{p_2})} \leq \frac{1 - p_1}{1 - p_2}$

暂无。

---

## 9.Entropy-based LSH

暂无。

---

## 参考资料:
- [1]: http://blog.csdn.net/zhou191954/article/details/8494438
- [2]: http://gemantic.iteye.com/blog/1701101
- [3]: http://www.jiahenglu.net/NSFC/LSH.html
- [4]:基于LSH的中文文本快速检索 2009.
- [5]:Weber R, Schek H, Blott S. A quantitative analysis and performance study for similarity search methods in high dimensional spaces Proc.of the 24th Intl.Conf.on Very Large Data Bases (VLDB).1998:194-205
- [6] Guttman A. R-trees: A dynamic index structure for spatial searching[C] Proc. of ACM Conf. on Management of Data (SIGMOD). 1984: 47-57
- [7] Bentley J L. K-D trees for semi-dynamic point sets[C] Proc. of the 6th ACM Symposium on Computational Geometry (SCG). 1990: 187-197
- [8] Ravichandran D, Pantel P, Hovy E. Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High S peed Noun Clustering[M]. Information Sciences Institute University of Southern California, 2004
- [9] Bawa M, Condie T, Ganesan P. LSH Forest: SelfTuning[C] Indexes for Similarity Search International World Wide Web Conference Proceedings of the 14th International Conference on World Wide Web. 2005
- [10] “Entropy based Nearest Neighbor Search in High Dimensions”. SODA 2006



